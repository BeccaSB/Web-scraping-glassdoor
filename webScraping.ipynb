{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c2d2e2-8509-49fa-b405-4efdf905a77b",
   "metadata": {},
   "source": [
    "# Web Scraping Glassdoor\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from Glassdoor, a popular job search platform. We'll use Python and BeautifulSoup, a Python library for web scraping, to extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "- Setup our development environment\n",
    "\n",
    "- Understand the basics of web scraping\n",
    "\n",
    "- Analyze the website structure of Glassdoor\n",
    "\n",
    "- Write the Python code to extract job data from Glassdoor\n",
    "\n",
    "- Save the data to a CSV file\n",
    "\n",
    "- Test our web scraper and refine our code as needed\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you'll need to install the following libraries in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "\n",
    "You can install them using the following command in your command prompt/terminal:\n",
    "\n",
    "**pip install requests**\n",
    "\n",
    "**pip install beautifulsoup4**\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "**Project Scope**\n",
    "\n",
    "We will be building a script that can scrape job postings from Glassdoor.com based on a specific job position and location. We will extract the following information from each job posting:\n",
    "\n",
    "Job title\n",
    "Company name\n",
    "Job location\n",
    "Posting date\n",
    "Summary of the job\n",
    "Salary (if available)\n",
    "Job URL\n",
    "We will then store this information in a CSV file for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc6b4d-ae09-4c5d-a385-0da3cbf79cf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "-\n",
    "-\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097524e-1e50-4fe8-93af-8e28d10bbea2",
   "metadata": {},
   "source": [
    "**Step 1: Importing Required Libraries**\n",
    "\n",
    "Here, we are importing the required libraries: csv for writing data to a CSV file, datetime for getting the current date, requests for sending HTTP requests to the website, BeautifulSoup for parsing the HTML source code of the webpage, and time for introducing a delay in our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6333af9b-6afe-43b6-ba19-b7f38c54d143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299588c-0a51-4621-b406-b2a6d1593035",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Step 2: Generating RL**\n",
    "\n",
    "Here, we are defining a function get_url that takes two parameters: position and location. We use these parameters to generate the URL of the webpage we want to scrape. We are using a template URL and replacing the placeholders with the actual values of position and location. The URL also includes some additional parameters such as locT=C and locId=1139970 that specify the location of the job posting. You can customize these parameters based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a798c8c2-d79b-411a-8a77-d163a398f462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_url(position, location):\n",
    "    \"\"\"Generate url from position and location\"\"\"\n",
    "    template = \"https://www.glassdoor.com/Job/jobs.htm?sc.keyword={}&locT=C&locId=1139970&JobType=all&fromAge=1\"\n",
    "    position = position.replace(\" \", \"+\")\n",
    "    location = location.replace(\" \", \"+\")\n",
    "    url = template.format(position, location)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fb431-b3d9-4936-a7f4-492c04338a14",
   "metadata": {},
   "source": [
    "**Step 3: Extracting Job Data**\n",
    "\n",
    "The next step is to define a function that will take a single job posting record as input, and extract the relevant data from it. This function will be called from within the main() function, which we will define in the next step.\n",
    "\n",
    "To do this, we'll use the BeautifulSoup library to parse the HTML of the job posting card, and extract the desired data using a series of try/except blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4adcec16-54a1-4da4-b33e-074ce06e73e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_record(card):\n",
    "    \"\"\"Extract job data from a single record\"\"\"\n",
    "    atags = card.find_all(\"a\")\n",
    "    try:\n",
    "        job_title = atags[0].text.strip()\n",
    "    except IndexError:\n",
    "        job_title = \"\"\n",
    "    try:\n",
    "        company = atags[1].text.strip()\n",
    "    except IndexError:\n",
    "        company = \"\"\n",
    "    try:\n",
    "        job_location = card.find(\"span\", {\"class\": \"jobLocation\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        job_location = \"\"\n",
    "    try:\n",
    "        post_date = card.find(\"span\", {\"class\": \"jobAge\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        post_date = \"\"\n",
    "    try:\n",
    "        summary = card.find(\"div\", {\"class\": \"jobDescriptionContent\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        summary = \"\"\n",
    "    try:\n",
    "        salary = card.find(\"span\", {\"class\": \"salaryText\"}).text.strip()\n",
    "    except AttributeError:\n",
    "        salary = \"\"\n",
    "    try:\n",
    "        job_url = \"https://www.glassdoor.com\" + atags[0][\"href\"]\n",
    "    except (IndexError, TypeError):\n",
    "        job_url = \"\"\n",
    "\n",
    "    today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    record = (job_title, company, job_location, post_date, today, summary, salary, job_url)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647185b-d1bd-43c1-b248-e8b20343af02",
   "metadata": {},
   "source": [
    "**Step 4: Define the manin function**\n",
    "\n",
    "Define the main function that takes two parameters: job position and location. This function performs the following steps:\n",
    "\n",
    "- Set the headers for the HTTP request. Glassdoor may block requests from bots, so it's a good idea to set a user agent string.\n",
    "- Construct the URL for the job search based on the job position and location.\n",
    "- Send an HTTP request to the URL and retrieve the HTML code of the search results page.\n",
    "- Parse the HTML code using BeautifulSoup and select the HTML elements that contain the job postings.\n",
    "- Extract the job posting information using the helper functions and store it in a list.\n",
    "- Write the job posting information to a CSV file.\n",
    "- Print a success message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0d8763da-f489-4fb5-a742-78953a5eca88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(position, location):\n",
    "    \"\"\"Run the main program routine\"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    url = get_url(position, location)\n",
    "    records = []\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        cards = soup.find_all(\"li\", {\"class\": \"react-job-listing\"})\n",
    "        for card in cards:\n",
    "            record = get_record(card)\n",
    "            records.append(record)\n",
    "        with open(\"jobs.csv\", \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    \"JobTitle\",\n",
    "                    \"Company\",\n",
    "                    \"JobLocation\",\n",
    "                    \"PostDate\",\n",
    "                    \"ExtractDate\",\n",
    "                    \"Summary\",\n",
    "                    \"Salary\",\n",
    "                    \"JobUrl\",\n",
    "                ]\n",
    "            )\n",
    "            writer.writerows(records)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error scraping job postings\")\n",
    "        return None\n",
    "    print(f\"Successfully scraped {len(records)} job postings\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c904a-325a-4380-9577-31e41593c24d",
   "metadata": {},
   "source": [
    "**Step 5: Run the Main Function**\n",
    "\n",
    "Call the main function with the job position and location parameters. Check the CSV file to verify that the job posting information has been extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "683fa0d0-27f3-4a9e-a34f-f45cd1fc1b27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 30 job postings\n"
     ]
    }
   ],
   "source": [
    "main('developer', 'texas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ee035-3f24-4ddf-8fa7-9c2f46545aeb",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Web scraping is a powerful tool for extracting data from websites. In this learning module, we have used web scraping to extract job postings from Glassdoor. You can use similar techniques to extract data from other websites as well. However, be aware of the legal and ethical implications of web scraping and make sure to comply with the website's terms of service.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb2dea-2442-40d9-83c7-18c25a376ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
